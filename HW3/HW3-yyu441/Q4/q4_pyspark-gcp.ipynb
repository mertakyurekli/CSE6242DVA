{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# HW3 Q4 [10 pts]\n\n\n\n## Important Notices\n\n<div class=\"alert alert-block alert-danger\">\n    WARNING: Do <strong>NOT</strong> add any cells to this Jupyter Notebook, because that will crash the autograder.\n</div>\n\n\nAll instructions, code comments, etc. in this notebook **are part of the assignment instructions**. That is, if there is instructions about completing a task in this notebook, that task is not optional.  \n\n\n\n<div class=\"alert alert-block alert-info\">\n    You <strong>must</strong> implement the following functions in this notebook to receive credit.\n</div>\n\n`user()`\n\n`load_data()`\n\n`exclude_no_pickuplocations()`\n\n`exclude_no_tripdistance()`\n\n`include_fare_range()`\n\n`get_highest_tip()`\n\n`get_total_toll()`\n\nEach method will be auto-graded using different sets of parameters or data, to ensure that values are not hard-coded.  You may assume we will only use your code to work with data from NYC Taxi Trips during auto-grading. You do not need to write code for unreasonable scenarios.  \n\nSince the overall correctness of your code will require multiple function to work together correctly (i.e., all methods are interdepedent), implementing only a subset of the functions likely will lead to a low score.\n\n### Helper functions\n\nYou are permitted to write additional helper functions, or use additional instance variables so long as the previously described functions work as required.  "}, {"cell_type": "markdown", "metadata": {}, "source": "#### Pyspark Imports\n<span style=\"color:red\">*Please don't modify the below cell*</span>"}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": "import pyspark\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import *"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Define Spark Context\n<span style=\"color:red\">*Please don't modify the below cell*</span>"}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [], "source": "sc\nsqlContext = SQLContext(sc)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Student Section - Please compete all the functions below"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Function to return GT Username"}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": "def user():\n        \"\"\"\n        :return: string\n        your GTUsername, NOT your 9-Digit GTId  \n        \"\"\"         \n        return 'yyu441'"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Function to load data"}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\ndef load_data(gcp_storage_path):\n    \"\"\"\n        :param gcp_storage_path: string (full gs path including file name e.g gs://bucket_name/data.csv) \n        :return: spark dataframe  \n    \"\"\"\n    \n    ################################################################\n    # code to load yellow_tripdata_2019-01.csv data from your GCP storage bucket#\n    #                                                              #        \n    ################################################################\n    # 'gs://yyu441/yellow_tripdata_2019-01.csv'\n    spark = SparkSession.builder.appName('GCSFilesRead').getOrCreate()\n    df = spark.read.csv(gcp_storage_path, header=True)\n    df = df.select(col('VendorID'),\n                   col('tpep_pickup_datetime'),\n                   col('tpep_dropoff_datetime'),\n                   col('passenger_count'),\n                   col('trip_distance').cast('Decimal(38,3)').alias('trip_distance'),\n                   col('RatecodeID'),\n                   col('store_and_fwd_flag'),\n                   col('PULocationID'),\n                   col('DOLocationID'),\n                   col('payment_type'),\n                   col('fare_amount').cast('Decimal(38,3)').alias('fare_amount'),\n                   col('extra'),\n                   col('mta_tax'),\n                   col('tip_amount').cast('Decimal(38,3)').alias('tip_amount'),\n                   col('tolls_amount').cast('Decimal(38,3)').alias('tolls_amount'),\n                   col('improvement_surcharge'),\n                   col('total_amount'))\n    return df"}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [], "source": "# df = load_data('gs://yyu441/yellow_tripdata_2019-01.csv')\n# df.show(2)"}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [], "source": "# df.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Function to exclude trips that don't have a pickup location"}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [], "source": "def exclude_no_pickuplocations(df):\n    \"\"\"\n        :param nyc tax trips dataframe: spark dataframe \n        :return: spark dataframe  \n    \"\"\"\n    \n    ################################################################\n    # code to exclude trips with no pickup locations               #\n    # Note: Exclude  nulls, blanks and zeros                       #        \n    ################################################################\n    # https://stackoverflow.com/questions/44163153/how-to-drop-rows-with-nulls-in-one-column-pyspark\n    df = df.filter(~(col('PULocationID').isNull() | isnan(col('PULocationID')) | (trim(col('PULocationID')) == \"\")))\n    return df"}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [], "source": "# df.filter(~(col('PULocationID').isNull() | isnan(col('PULocationID')) | (trim(col('PULocationID')) == \"\"))).count()"}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+----+----+\n|  _1|  _2|  _3|\n+----+----+----+\n| foo|null| 3.0|\n| bar|   1| NaN|\n|good|  42|42.0|\n+----+----+----+\n\n"}, {"data": {"text/plain": "DataFrame[sum(_2): bigint]"}, "execution_count": 43, "metadata": {}, "output_type": "execute_result"}], "source": "t = spark.createDataFrame([\n    (\"\", 1, 2.0), \n    (\"foo\", None, 3.0),\n    (\"bar\", 1, float(\"NaN\")), \n    (\"good\", 42, 42.0)])\nt.filter(~(col('_1').isNull() | isnan(col('_1')) | (trim(col('_1')) == \"\"))).show()\nt.groupBy().sum('_2')"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Function to exclude trips with no distance"}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [], "source": "def exclude_no_tripdistance(df):\n    \"\"\"\n        :param nyc tax trips dataframe: spark dataframe \n        :return: spark dataframe  \n    \"\"\"\n    \n    ################################################################\n    # code to exclude trips with no trip distances                 #\n    # Note: Exclude  nulls, blanks and zero                        #        \n    ################################################################\n    df = df.filter(~(col('trip_distance').isNull() | isnan(col('trip_distance')) | (trim(col('trip_distance')) == \"\") | (col('trip_distance') == 0)))\n    return df"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Function to include fare amount between the range of 20 to 60 Dollars"}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [], "source": "def include_fare_range(df):\n    \"\"\"\n        :param nyc tax trips dataframe: spark dataframe \n        :return: spark dataframe  \n    \"\"\"\n    \n    ################################################################\n    # code to include trips with only within the fare range of     #\n    # 20 to 60 dollars (including 20 and 60 dollars)               #        \n    ################################################################\n    df = df.filter((col('fare_amount') >= 20.0) & (col('fare_amount') <= 60.0))\n    return df"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Function to get the highest tip amount"}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [], "source": "def get_highest_tip(df):\n    \"\"\"\n        :param nyc tax trips dataframe: spark dataframe \n        :return: decimal (rounded to 2 digits)  (NOTE: DON'T USE FLOAT)\n    \"\"\"\n    \n    ################################################################\n    # code to get the highest tip                                  #\n    #                                                              #        \n    ################################################################\n    max_tip = df.withColumn('tip_amount', round('tip_amount', 2)).select('tip_amount').orderBy(col('tip_amount').desc()).limit(1).first()[0]\n    return max_tip"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Function to get total toll amount"}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [], "source": "def get_total_toll(df):\n    \"\"\"\n        :param nyc tax trips dataframe: spark dataframe \n        :return: decimal (rounded to 2 digits)  (NOTE: DON'T USE FLOAT)\n    \"\"\"\n    \n    ################################################################\n    # code to get total toll amount                                #\n    #                                                              #        \n    ################################################################\n    t = df.groupBy().agg((sum('tolls_amount').cast(\"decimal(38, 10)\")).alias('total_tolls'))\n    total_toll = t.withColumn('total_tolls', round('total_tolls', 2)).first()[0]\n    return total_toll"}, {"cell_type": "markdown", "metadata": {}, "source": "### Run above functions and print\n\n#### Uncomment the cells below and test your implemented functions"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Load data from yellow_tripdata_2019-01.csv "}, {"cell_type": "code", "execution_count": 48, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- VendorID: string (nullable = true)\n |-- tpep_pickup_datetime: string (nullable = true)\n |-- tpep_dropoff_datetime: string (nullable = true)\n |-- passenger_count: string (nullable = true)\n |-- trip_distance: decimal(38,3) (nullable = true)\n |-- RatecodeID: string (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: string (nullable = true)\n |-- DOLocationID: string (nullable = true)\n |-- payment_type: string (nullable = true)\n |-- fare_amount: decimal(38,3) (nullable = true)\n |-- extra: string (nullable = true)\n |-- mta_tax: string (nullable = true)\n |-- tip_amount: decimal(38,3) (nullable = true)\n |-- tolls_amount: decimal(38,3) (nullable = true)\n |-- improvement_surcharge: string (nullable = true)\n |-- total_amount: string (nullable = true)\n\n"}], "source": "gcp_storage_path = \"gs://yyu441/yellow_tripdata_2019-01.csv\"\ndf = load_data(gcp_storage_path)\ndf.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Print total numbers of rows in the dataframe"}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [{"data": {"text/plain": "7667792"}, "execution_count": 49, "metadata": {}, "output_type": "execute_result"}], "source": "df.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Print total number of rows in the dataframe after excluding trips with no pickup location"}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [{"data": {"text/plain": "7667792"}, "execution_count": 50, "metadata": {}, "output_type": "execute_result"}], "source": "df_no_pickup_locations = exclude_no_pickuplocations(df)\ndf_no_pickup_locations.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Print total number of rows in the dataframe after exclude trips with no distance"}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [{"data": {"text/plain": "7613022"}, "execution_count": 51, "metadata": {}, "output_type": "execute_result"}], "source": "df_no_trip_distance = exclude_no_tripdistance(df)\ndf_no_trip_distance.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Print total number of rows in the dataframe after including trips with fair amount between the range of 20 to 60 Dollars"}, {"cell_type": "code", "execution_count": 52, "metadata": {}, "outputs": [{"data": {"text/plain": "969417"}, "execution_count": 52, "metadata": {}, "output_type": "execute_result"}], "source": "df_include_fare_range = include_fare_range(df)\ndf_include_fare_range.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Print the highest tip amount"}, {"cell_type": "code", "execution_count": 53, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "787.25\n"}], "source": "max_tip = get_highest_tip(df)\nprint(max_tip)"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Print the total toll amount"}, {"cell_type": "code", "execution_count": 54, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "2430066.70\n"}], "source": "total_toll = get_total_toll(df)\nprint(total_toll)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}